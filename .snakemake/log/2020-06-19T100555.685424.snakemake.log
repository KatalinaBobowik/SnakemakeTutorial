Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 1
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	2	map_reads
	2	sort
	4

[Fri Jun 19 10:05:55 2020]
rule map_reads:
    input: data/genome.fa, data/samples/B.fastq
    output: mapped/B.bam
    jobid: 2
    wildcards: sample=B

Activating conda environment: /home/kbobowik/snakemake-tutorial/.snakemake/conda/1e5f4063
[Fri Jun 19 10:05:57 2020]
Finished job 2.
1 of 4 steps (25%) done

[Fri Jun 19 10:05:57 2020]
rule map_reads:
    input: data/genome.fa, data/samples/A.fastq
    output: mapped/A.bam
    jobid: 3
    wildcards: sample=A

Activating conda environment: /home/kbobowik/snakemake-tutorial/.snakemake/conda/1e5f4063
[Fri Jun 19 10:05:59 2020]
Finished job 3.
2 of 4 steps (50%) done

[Fri Jun 19 10:05:59 2020]
rule sort:
    input: mapped/B.bam
    output: mapped/B.sorted.bam
    jobid: 0
    wildcards: sample=B

Activating conda environment: /home/kbobowik/snakemake-tutorial/.snakemake/conda/1e5f4063
[Fri Jun 19 10:05:59 2020]
Finished job 0.
3 of 4 steps (75%) done

[Fri Jun 19 10:05:59 2020]
rule sort:
    input: mapped/A.bam
    output: mapped/A.sorted.bam
    jobid: 1
    wildcards: sample=A

Activating conda environment: /home/kbobowik/snakemake-tutorial/.snakemake/conda/1e5f4063
[Fri Jun 19 10:06:00 2020]
Finished job 1.
4 of 4 steps (100%) done
Complete log: /home/kbobowik/snakemake-tutorial/.snakemake/log/2020-06-19T100555.685424.snakemake.log
