Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 1
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	2	map_reads
	2	sort
	4

[Wed Jun 17 21:19:47 2020]
rule map_reads:
    input: data/genome.fa, data/samples/B.fastq
    output: mapped/B.bam
    jobid: 3
    wildcards: sample=B

Activating conda environment: /home/kbobowik/snakemake-tutorial/.snakemake/conda/1e5f4063
[Wed Jun 17 21:19:49 2020]
Finished job 3.
1 of 4 steps (25%) done

[Wed Jun 17 21:19:49 2020]
rule map_reads:
    input: data/genome.fa, data/samples/A.fastq
    output: mapped/A.bam
    jobid: 2
    wildcards: sample=A

Activating conda environment: /home/kbobowik/snakemake-tutorial/.snakemake/conda/1e5f4063
[Wed Jun 17 21:19:50 2020]
Finished job 2.
2 of 4 steps (50%) done

[Wed Jun 17 21:19:50 2020]
rule sort:
    input: mapped/B.bam
    output: mapped/B.sorted.bam
    jobid: 1
    wildcards: sample=B

Activating conda environment: /home/kbobowik/snakemake-tutorial/.snakemake/conda/1e5f4063
[Wed Jun 17 21:19:51 2020]
Finished job 1.
3 of 4 steps (75%) done

[Wed Jun 17 21:19:51 2020]
rule sort:
    input: mapped/A.bam
    output: mapped/A.sorted.bam
    jobid: 0
    wildcards: sample=A

Activating conda environment: /home/kbobowik/snakemake-tutorial/.snakemake/conda/1e5f4063
[Wed Jun 17 21:19:51 2020]
Finished job 0.
4 of 4 steps (100%) done
Complete log: /home/kbobowik/snakemake-tutorial/.snakemake/log/2020-06-17T211947.388161.snakemake.log
